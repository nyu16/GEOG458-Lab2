{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtnaSlH-wplB"
      },
      "outputs": [],
      "source": [
        "# created on Dec 24, 2020\n",
        "# modified on April 12, 2022\n",
        "# @author:          Nicholas Yu\n",
        "# @email:           nyu16@uw.edu\n",
        "# @website:         https://hgis.uw.edu\n",
        "# @organization:    Department of Geography, University of Washington, Seattle\n",
        "# @description:     Search geo-tagged tweets within the U.S. This script is modified from https://github.com/shawn-terryah/Twitter_Geolocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f-bMdKMawxnQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "945869c9-531c-4eca-ffe0-5ba5d4886c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "import tweepy, json, time\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "# Create data on to Google Drive\n",
        "from google.colab import drive\n",
        "# Mount your Drive to the Colab VM.\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dEyPts-ZwplE"
      },
      "outputs": [],
      "source": [
        "class StreamListener(tweepy.StreamListener):\n",
        "    \"\"\"tweepy.StreamListener is a class provided by tweepy used to access\n",
        "    the Twitter Streaming API to collect tweets in real-time.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, time_limit=60, file=\"\"):\n",
        "        \"\"\"class initialization\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        self.limit = time_limit\n",
        "        self.result = []\n",
        "        self.f = file\n",
        "        super(StreamListener, self).__init__()\n",
        "\n",
        "    def on_data(self, data):\n",
        "        \"\"\"This is called when data are streamed in.\"\"\"\n",
        "        if (time.time() - self.start_time) < self.limit:\n",
        "            datajson = json.loads(data)\n",
        "            # print(datajson, \"\\n\")\n",
        "            if 'id' not in datajson.keys():\n",
        "                time.sleep(10)\n",
        "            else:\n",
        "                id = datajson['id']\n",
        "                username = datajson['user']['screen_name']\n",
        "                created_at = datajson['created_at']\n",
        "                text = datajson['text'].strip().replace(\"\\n\", \"\")\n",
        "\n",
        "                # process the geo-tags\n",
        "                if datajson['coordinates'] == None:\n",
        "                    try:\n",
        "                        bbox = datajson['place']['bounding_box']['coordinates'][0]\n",
        "                        lng = (bbox[0][0] + bbox[2][0]) / 2.0\n",
        "                        lat = (bbox[0][1] + bbox[1][1]) / 2.0\n",
        "                    except:\n",
        "                        lat = 0\n",
        "                        lng = 0\n",
        "                else:\n",
        "                    lng = datajson['coordinates']['coordinates'][0]\n",
        "                    lat = datajson['coordinates']['coordinates'][1]\n",
        "\n",
        "                if lat != 0:\n",
        "                    row = {\n",
        "                        'id': id,\n",
        "                        'username': username,\n",
        "                        'created_at': created_at,\n",
        "                        'lng': lng,\n",
        "                        'lat': lat,\n",
        "                        'text': text\n",
        "                    }\n",
        "                    print(row)\n",
        "                    self.result.append(row)\n",
        "                else:\n",
        "                    pass\n",
        "        else:\n",
        "            df = pd.DataFrame(self.result)\n",
        "            df.to_csv(self.f, index=False)\n",
        "            # download the csv to your local computer\n",
        "            files.download(self.f)\n",
        "            print(\"the csv has been downloaded to your local computer. The program has been completed successfully.\")\n",
        "            return False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def on_data(self, data):\n",
        "    \"\"\"This is called when data are streamed in.\"\"\"\n",
        "    if (time.time() - self.start_time) < self.limit:\n",
        "        datajson = json.loads(data)\n",
        "        print(datajson, \"\\n\")\n",
        "        if 'id' not in datajson.keys():\n",
        "            time.sleep(10)\n",
        "        else:\n",
        "            id = datajson['id']\n",
        "            username = datajson['user']['screen_name']\n",
        "            created_at = datajson['created_at']\n",
        "            text = datajson['text'].strip().replace(\"\\n\", \"\")\n",
        "\n",
        "            # process the geo-tags\n",
        "            if datajson['coordinates'] == None:\n",
        "                try:\n",
        "                    bbox = datajson['place']['bounding_box']['coordinates'][0]\n",
        "                    lng = (bbox[0][0] + bbox[2][0]) / 2.0\n",
        "                    lat = (bbox[0][1] + bbox[1][1]) / 2.0\n",
        "                except:\n",
        "                    lat = 0\n",
        "                    lng = 0\n",
        "            else:\n",
        "                lng = datajson['coordinates']['coordinates'][0]\n",
        "                lat = datajson['coordinates']['coordinates'][1]\n",
        "\n",
        "            if lat != 0:\n",
        "                row = {\n",
        "                    'id': id,\n",
        "                    'username': username,\n",
        "                    'created_at': created_at,\n",
        "                    'lng': lng,\n",
        "                    'lat': lat,\n",
        "                    'text': text\n",
        "                }\n",
        "                print(row)\n",
        "                self.result.append(row)\n",
        "            else:\n",
        "                pass\n",
        "    else:\n",
        "        df = pd.DataFrame(self.result)\n",
        "        df.to_csv(self.f, index=False)\n",
        "        # download the csv to your local computer\n",
        "        files.download(self.f)\n",
        "        print(\"the csv has been downloaded to your local computer. The program has been completed successfully.\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "JsGsQq-JKMrx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5Xzyo_OiwplG"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # These are provided to you through the Twitter API after you create a account\n",
        "    # register a Twitter App to get the keys and access tokens.\n",
        "    output_file = '/gdrive/My Drive/denver.csv'\n",
        "\n",
        "    # Apply for your own Twitter API keys at https://developer.twitter.com/en/apply-for-access\n",
        "    consumer_key = \"----------\"\n",
        "    consumer_secret = \"----------\"\n",
        "    access_token = \"-----------\"\n",
        "    access_token_secret = \"----------\"\n",
        "\n",
        "    myauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    myauth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "    # LOCATIONS are the longitude, latitude coordinate corners for a box that restricts the\n",
        "    # geographic area from which you will stream tweets. The first two define the southwest\n",
        "    # corner of the box and the second two define the northeast corner of the box.\n",
        "    LOCATIONS = [-124.7771694, 24.520833, -66.947028, 49.384472,  # Contiguous US\n",
        "                 -164.639405, 58.806859, -144.152365, 71.76871,  # Alaska\n",
        "                 -160.161542, 18.776344, -154.641396, 22.878623]  # Hawaii\n",
        "\n",
        "    stream_listener = StreamListener(time_limit=600, file=output_file)\n",
        "    stream = tweepy.Stream(auth=myauth, listener=stream_listener)\n",
        "    stream.filter(locations=LOCATIONS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "search_words = \"Climate Change\"\n",
        "# make sure there is no space between lat, long and the radius.\n",
        "location = \"39.734690923217585,-104.94575841060016,10mi\"\n",
        "\n",
        "date_since = \"2023-1-14\"\n",
        "\n",
        "# Collect tweets\n",
        "tweets = tweepy.Cursor(api.search, q=search_words, geocode=location, lang=\"en\", since=date_since).items(1000)"
      ],
      "metadata": {
        "id": "GWik7oihKXgw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an array to store the result\n",
        "result = []\n",
        "\n",
        "# Iterate and print tweets\n",
        "for tweet in tweets:\n",
        "    row = {\n",
        "        'username': tweet.author.name,\n",
        "        'userid': tweet.author.id,\n",
        "        'profile_location': tweet.author.location,\n",
        "        'created_at': str(tweet.author.created_at),\n",
        "        'text': tweet.text,\n",
        "        'retweet_count': tweet.retweet_count,\n",
        "        'source': tweet.source,\n",
        "        'coordinates': tweet.coordinates\n",
        "    }\n",
        "    result.append(row)\n",
        "    print(row)\n",
        "\n",
        "# Store the results as a pandas dataframe\n",
        "df = pd.DataFrame(result)\n",
        "\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "# notify the completion of the crawling in the console.\n",
        "print(\"the crawling task is finished.\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(output_file)\n",
        "print(\"the csv has been downloaded to your local computer. The program has been completed successfully.\")"
      ],
      "metadata": {
        "id": "3ZAOMHPBSPXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = '/gdrive/My Drive/vancouver.csv'\n",
        "\n",
        "va_search_words = \"Climate Change\"\n",
        "# make sure there is no space between lat, long and the radius.\n",
        "vancouver = \"49.24807823416991,-123.1208158910047,10mi\"\n",
        "\n",
        "# Collect tweets\n",
        "tweets = tweepy.Cursor(api.search, q=va_search_words, geocode=vancouver, lang=\"en\", since=date_since).items(1000)\n",
        "\n",
        "# create an array to store the result\n",
        "vanc = []\n",
        "\n",
        "# Iterate and print tweets\n",
        "for tweet in tweets:\n",
        "    row = {\n",
        "        'username': tweet.author.name,\n",
        "        'userid': tweet.author.id,\n",
        "        'profile_location': tweet.author.vancouver,\n",
        "        'created_at': str(tweet.author.created_at),\n",
        "        'text': tweet.text,\n",
        "        'retweet_count': tweet.retweet_count,\n",
        "        'source': tweet.source,\n",
        "        'coordinates': tweet.coordinates\n",
        "    }\n",
        "    vanc.append(row)\n",
        "    print(row)\n",
        "\n",
        "# Store the results as a pandas dataframe\n",
        "df = pd.DataFrame(vanc)\n",
        "\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "# notify the completion of the crawling in the console.\n",
        "print(\"the crawling task is finished.\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(output_file)\n",
        "print(\"the csv has been downloaded to your local computer. The program has been completed successfully.\")"
      ],
      "metadata": {
        "id": "_QqlXq1JajGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = '/gdrive/My Drive/seattle.csv'\n",
        "\n",
        "se_search_words = \"Climate Change\"\n",
        "# make sure there is no space between lat, long and the radius.\n",
        "seattle = \"47.62433332806439,-122.31629473227378,10mi\"\n",
        "\n",
        "# Collect tweets\n",
        "tweets = tweepy.Cursor(api.search, q=se_search_words, geocode=seattle, lang=\"en\", since=date_since).items(1000)\n",
        "\n",
        "# create an array to store the result\n",
        "seat = []\n",
        "\n",
        "# Iterate and print tweets\n",
        "for tweet in tweets:\n",
        "    row = {\n",
        "        'username': tweet.author.name,\n",
        "        'userid': tweet.author.id,\n",
        "        'profile_location': tweet.author.seattle,\n",
        "        'created_at': str(tweet.author.created_at),\n",
        "        'text': tweet.text,\n",
        "        'retweet_count': tweet.retweet_count,\n",
        "        'source': tweet.source,\n",
        "        'coordinates': tweet.coordinates\n",
        "    }\n",
        "    seat.append(row)\n",
        "    print(row)\n",
        "\n",
        "# Store the results as a pandas dataframe\n",
        "df = pd.DataFrame(seat)\n",
        "\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "# notify the completion of the crawling in the console.\n",
        "print(\"the crawling task is finished.\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(output_file)\n",
        "print(\"the csv has been downloaded to your local computer. The program has been completed successfully.\")"
      ],
      "metadata": {
        "id": "kLSV384hajcA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}